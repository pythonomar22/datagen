"""
DataGen Metrics Module

This module provides tools for evaluating the quality, diversity, and usefulness
of synthetic datasets generated by the DataGen library.
"""

import logging
from typing import Dict, Any, List, Optional, Tuple, Union
import os

from datagen.results import Results
from datagen.config import Config

# Import all submodules
from .basic_stats import compute_basic_stats
from .diversity import compute_diversity_metrics
from .perplexity import compute_perplexity
from .readability import compute_readability_scores
from .topic_modeling import extract_topics
from .visualization import (
    create_length_distribution_plot,
    create_topic_distribution_plot,
    create_readability_plot,
    create_diversity_plot,
    create_perplexity_plot,
    create_summary_dashboard
)

logger = logging.getLogger(__name__)


def evaluate(
    dataset: Results,
    config: Optional[Config] = None,
    output_dir: Optional[str] = None,
    compute_perplexity: bool = True,
    compute_topics: bool = True,
    create_visualizations: bool = True,
    visualization_format: str = "png"
) -> Dict[str, Any]:
    """
    Evaluate a dataset using multiple metrics to assess quality, diversity, and usefulness.
    
    Args:
        dataset: Results object containing the dataset to evaluate
        config: Optional Config object with metrics parameters
        output_dir: Directory to save visualization files. If None, visualizations are not saved to disk.
        compute_perplexity: Whether to compute perplexity scores (can be slow)
        compute_topics: Whether to extract topics from the dataset
        create_visualizations: Whether to generate visualization plots
        visualization_format: Format for saved visualization files ('png', 'pdf', 'svg')
        
    Returns:
        Dictionary containing all computed metrics
    """
    if len(dataset) == 0:
        logger.warning("Empty dataset provided for evaluation. Returning empty metrics.")
        return {"error": "Empty dataset"}
    
    # Create output directory if needed
    if output_dir and create_visualizations:
        os.makedirs(output_dir, exist_ok=True)
        logger.info(f"Visualizations will be saved to {output_dir}")
    
    # Initialize results dictionary
    metrics = {}
    
    # Compute basic statistics
    logger.info("Computing basic statistics...")
    metrics["basic_stats"] = compute_basic_stats(dataset)
    
    # Compute diversity metrics
    logger.info("Computing diversity metrics...")
    metrics["diversity"] = compute_diversity_metrics(dataset)
    
    # Compute perplexity if requested
    if compute_perplexity:
        try:
            logger.info("Computing perplexity scores...")
            metrics["perplexity"] = compute_perplexity(dataset)
        except Exception as e:
            logger.warning(f"Failed to compute perplexity: {str(e)}")
            metrics["perplexity"] = {"error": str(e)}
    
    # Compute readability scores
    logger.info("Computing readability scores...")
    metrics["readability"] = compute_readability_scores(dataset)
    
    # Extract topics if requested
    if compute_topics:
        try:
            logger.info("Extracting topics...")
            metrics["topics"] = extract_topics(dataset)
        except Exception as e:
            logger.warning(f"Failed to extract topics: {str(e)}")
            metrics["topics"] = {"error": str(e)}
    
    # Generate visualizations if requested
    if create_visualizations:
        logger.info("Generating visualizations...")
        metrics["visualizations"] = {}
        
        try:
            # Length distribution plot
            length_plot_path = None
            if output_dir:
                length_plot_path = os.path.join(output_dir, f"length_distribution.{visualization_format}")
            fig_length = create_length_distribution_plot(dataset, metrics["basic_stats"], save_path=length_plot_path)
            metrics["visualizations"]["length_distribution"] = length_plot_path
            
            # Topic distribution plot if topics were extracted
            if compute_topics and "topics" in metrics and "error" not in metrics["topics"]:
                topic_plot_path = None
                if output_dir:
                    topic_plot_path = os.path.join(output_dir, f"topic_distribution.{visualization_format}")
                fig_topic = create_topic_distribution_plot(metrics["topics"], save_path=topic_plot_path)
                metrics["visualizations"]["topic_distribution"] = topic_plot_path
            
            # Readability plot
            readability_plot_path = None
            if output_dir:
                readability_plot_path = os.path.join(output_dir, f"readability_scores.{visualization_format}")
            fig_readability = create_readability_plot(metrics["readability"], save_path=readability_plot_path)
            metrics["visualizations"]["readability_scores"] = readability_plot_path
            
            # Diversity plot
            diversity_plot_path = None
            if output_dir:
                diversity_plot_path = os.path.join(output_dir, f"diversity_metrics.{visualization_format}")
            fig_diversity = create_diversity_plot(metrics["diversity"], save_path=diversity_plot_path)
            metrics["visualizations"]["diversity_metrics"] = diversity_plot_path
            
            # Perplexity plot if perplexity was computed
            if compute_perplexity and "perplexity" in metrics and "error" not in metrics["perplexity"]:
                perplexity_plot_path = None
                if output_dir:
                    perplexity_plot_path = os.path.join(output_dir, f"perplexity.{visualization_format}")
                fig_perplexity = create_perplexity_plot(metrics["perplexity"], save_path=perplexity_plot_path)
                metrics["visualizations"]["perplexity"] = perplexity_plot_path
            
            # Create a summary dashboard with all metrics
            summary_path = None
            if output_dir:
                summary_path = os.path.join(output_dir, f"metrics_summary.{visualization_format}")
            fig_summary = create_summary_dashboard(metrics, save_path=summary_path)
            metrics["visualizations"]["summary_dashboard"] = summary_path
            
        except Exception as e:
            logger.warning(f"Failed to generate some visualizations: {str(e)}")
            metrics["visualizations"]["error"] = str(e)
    
    # Return all computed metrics
    return metrics 